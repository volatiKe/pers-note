# 海量数据处理

---

常用手段：

* 分片：
  * hash 取模
  * 标签
* 堆排序
* 外部排序
* 桶排序
* Bitmap
* Bloom Filter

## 1. TOP-K

**海量日志文件中统计最热门的 K 个搜索关键词**：

* hash(key) % N 将文件分片
* 针对每片数据使用 HashMap 统计次数
* 使用堆选出 TOP-K

**日志文件分散在多个机器上，统计最热门的 K 个搜索关键词**：

* 如果关键词在不同机器上存在重复，可以使用 hash 取模的方式将数据重新映射到多台机器上，此时重复的元素一定在同一台机器上
* 使用 HashMap 和堆即可

## 2. 排序

**多个日志文件，按照搜索次数排序**：

* 重新 hash 取模，将相同元素写入同一个文件
* 使用 HashMap 统计次数
* 外部排序

**上亿数字中找出中位数**：

* 划分多个桶，遍历数字放入对应的桶中
* 统计各个桶中数字的个数即可知中位数所处的桶

## 3. 判重

**两个文件，各存放海量 URL，找出两个文件中共同的 URL**：

* 将一个文件的 URL 读入 Bloom Filter
* 遍历另一个文件的 URL，使用 Bloom Filter 判重

**Bloom Filter 存在一定误判率，如果不允许误判**：

* 将两个文件的 URL 分别进行 hash 取模，分散成多个文件，这样相同的 URL 一定出现在同一对分散后的文件中
* 对于一对文件：
  * 遍历一个文件使用 HashMap 计数，再遍历另一个即可
  * 将一个文件的 URL 存入 set

**上亿数字中找出不重复的数字**：

* 使用 2 Bitmap，每个数分配 2bit：
  * 00：不存在
  * 01：一次
  * 10：多次

## 4. 模糊查找

根据相似度在海量图片库中查找：

* 考虑使用多维度标签的方式将图片数据分片
* 单片图片数据下使用堆来查找 TOP-K 相似度的图片

## 1. CAP

CAP 指一致性（Consistency）、可用性（Availability）、分区容忍性（Partition Tolerance），分布式系统的这三者无法同时满足

* C：每次请求读到的都是最新的数据或错误信息
* A：每次请求都能获得非错误的响应，但不能保证数据是最新的
* P：节点间丢失数据或节点不可用，系统还可以对外提供服务

> 如何理解 CAP 的「三选二」？
> 保存数据的节点越多， P 就越高，但需同步的数据就越多，C 就越难保证，而为了保证 C，每次同步数据就需要等待各个节点的统一，会导致 A 的降低

* CP：zookeeper，任何时刻对 zookeeper 的访问都能得到一致的结果
* AP：
	* 高并发的网站，如 12306
	* 集群模式下的 redis，节点故障时可能会存在数据的不一致

### 1.1 一致性

* 强一致性：单机下的 MySQL，由 ACID 保证强一致性
* 弱一致性：数据库和缓存的一致性
* 最终一致性：基于本地消息表的分布式事务

### 1.2 可用性

* BA（Basically Available）：基本可用，就是说分布式系统在核心可用下允许部分故障
* S（Soft State）：软状态，不影响系统整体可用性下允许数据存在中间状态，如数据正在同步
* E（Eventually Consistency）：最终一致性

#### 双机房实时热备

* redis

* mysql

### 1.3 分区容忍性

* 一致性 hash 解决扩缩容问题
* 提升基础设施的稳定性

## 2. 分布式 ID

* 雪花模式：
	* 有时钟漂移问题：如果获取的当前时间比成员变变量记录的时间小就抛异常
* 号段模式：
	* 每次从 db 中获取一批 id 并放在缓存中，下次使用 id 时从缓存中获取
	* 每个 id 服务的实例获取不同号段的 id
	* 避免了时钟漂移问题
	* 但多个缓存刚好同时用尽号段去 db 中获取时可能会并发争抢导致性能问题（可以消耗到一定阈值时异步去更新缓存中号段）

> 如何进一步提升性能？
> * 给全局 ID 生成器安装多个前置发号器，每个前置发号器发送特定范围内的 ID
> * 直接使用多个 ID 生成器，每个生成器按照一定规则 ID，互不干扰

## 3. 分布式缓存

### 3.1 缓存负载策略

一致性 hash 算法：

* 场景：n 台机器缓存数据，使用 `hash() % n` 的方式将数据分散存储，在访问数据时使用同样的方式可直接获取到目标机器
* 问题：机器数量变动，大量缓存同时失效，出现缓存雪崩
* **hash 环**：将 hash 空间 [0, 2^32-1] 看作一个环，使用 `hash（机器唯一标识） % 2^32` 得到的机器在 hash 环中的位置，同理也能得到数据在 hash 环中的位置，数据存储在顺时针方向遇到的第一台机器中。这种方式使得在机器数量变动时，只有少部分的缓存会失效，避免了缓存雪崩
* **hash 环的偏斜**：机器在 hash 环上的分布不均匀，导致大量的缓存集中在某几台机器上，这时可以使用虚拟节点的方式，将物理机器映射成均匀分布在环中的多个虚拟节点，使得缓存分布均匀

### 3.2 缓存读写一致性策略

需要明确的是，如果对数据库有强一致性要求，那么就不能使用缓存，Redis 只能保证最终一致性

#### 3.2.1 Cache Aside

* 先操作 db，再操作缓存
* 适合读多写少，因为写多时缓存会频繁被删除，影响缓存命中率

##### 删除而代替更新

* 删除相较于更新更简单，但会造成一次 cache miss，极限情况下会造成缓存击穿
* 更新缓存在写写并发下会有问题：
	1. A 写缓存 10｜A 写 db 10
	2. B 写缓存 20｜B 写 db 20
	3. B 写 db 20｜B 写缓存 20
	4. A 写 db 10｜A 写缓存 10

##### 先更新数据库再删缓存（Cache Aside）

问题：
* 删除缓存失败
解决：
* 可以使用较短的过期时间
* 删除失败后默认重试一次，重试失败则写入 mq，缓存组件恢复后消费消息删掉这些缓存，删除失败再重复此流程

##### 先删缓存在更新数据库

问题：
* 高并发中放大不一致问题
	1. A 删缓存
	2. B cache miss 读 db 为 20
	3. B 更新 cache 为 20
	4. A 更新 db 为 21
解决：
* 再删一次

##### 选择

cache aside 由于可能删除失败，所以在并发不高时可以使用，但高并发下希望尽可能减少数据不一致问题，就可以采用延迟双删的策略，更复杂，但并发问题更少一些

#### 3.2.2 Read/Write Through

应用程序直接和缓存打交道，缓存组件自行维护缓存和数据库的一致性，本地缓存可以使用

#### 3.2.3 Write Back

* 只更新缓存不更新数据库
* 异步将数据刷新回数据库

适合写多的场景，速度快，但会造成一定程度的数据丢失

### 3.3 缓存淘汰策略

#### 3.3.1 LRU

* 基本数据结构
	* map1：根据 key O(1) 获取 value
	* map2：根据 key O(1) 获取 node
	* 双向链表：O(1) 做删除
* put：
	* 如果 key 存在，则删掉 node，移动至头部，map 更新 value
	* 如果缓存满，则删掉 last node，新 node 头插，map 增加 kv
	* 否则新 node 头插，map 增加 kv
* get：
	* 获取 value
	* 执行 key 存在时的 put

> LRU 的问题：无法解决缓存污染，比如一次读了大量缓存数据，然后很久没有再访问过

#### 3.3.2 LFU

* 基本数据结构
	* mapK2V：根据 key O(1) 获取 value
	* mapK2F：根据 key O(1) 获取 freq
	* minFreq：记录当前最小 freq
	* mapF2K：
		* 根据 key O(1) 获取 freq 对应的 key 集合
		* value 为 LinkedHashSet
			* key 有序，可以 O(1) 删除最不常使用的 key
			* 快速定位 key，因为某个 key 被访问时，freq 会 +1，就需要从该 freq 对应的 key 集合中删除
* put：
	* 如果 key 存在，增加 freq
		* 从 freq 对应的 key 集合删除该 key
		* 在 freq + 1 对应的 key 集合增加该 key
		* 更新 minFreq
	* 如果缓存满，删除 minFreq 对应 key 集合中最早访问的 key
		* mapK2V & mapK2F 删除该 key
	* 否则保存该 kv
		* minFreq 置 1
		* 三个 map 增加此 key
* get：
	* 获取 value
	* 执行 key 存在时的 put

## 分布式事务

### 基于本地消息表

#### producer

1. 本地业务操作和写生产者表在同一个事务
2. 发消息
	1. 发消息失败则靠生产者表补单
3. 发消息成功后修改生产者表消息状态
	1. 修改生产者表消息状态失败则靠生产者表补单

#### consumer

消费者有多个，各自的业务表不一定和消费者表在同一个库

1. 拉消息
2. 查询消费者表是否有数据
	1. 如果有且为成功，则直接返回
	2. 如果没有则业务执行消费逻辑
		1. 业务需要自己的表控制消息幂等消费
3. 如果业务处理失败，则写初始态的消费者表，消费者表中有消息体，补单重发消息
4. 如果业务处理成功，则写成功态的消费者表
5. 提交偏移量
	1. 如果写消费者表失败，则不会提交偏移量

### 简化的 TCC

1. 查询可购买人数
2. 请求冻结人数
3. 冻结人数
	1. 写 prepare 的人数扣减 log
4. 建交易单
	1. 如果交易失败则请求解冻人数
	2. 解冻失败则由 CT 根据 prepare 的 log 进行补偿
5. 请求扣减人数
	1. prepare 推到 commit